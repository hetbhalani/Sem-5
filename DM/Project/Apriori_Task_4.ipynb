{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1422b500",
   "metadata": {},
   "source": [
    "# Apriori Algorithm Implementation Assignment\n",
    "\n",
    "### Objective:\n",
    "You will implement the **Apriori algorithm** from scratch (i.e., without using any libraries like `mlxtend`) to find frequent itemsets and generate association rules.\n",
    "\n",
    "### Dataset:\n",
    "Use the [Online Retail Dataset](https://www.kaggle.com/datasets/vijayuv/onlineretail) from Kaggle. You can filter it for a specific country (e.g., `United Kingdom`) and time range to reduce size if needed.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85128a0",
   "metadata": {},
   "source": [
    "## Step 1: Data Preprocessing\n",
    "\n",
    "- Load the dataset\n",
    "- Remove rows with missing values\n",
    "- Filter out rows where `Quantity <= 0`\n",
    "- Convert Data into Basket Format\n",
    "\n",
    "ðŸ‘‰ **Implement code below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8054989",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31ab0268",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./Datasets/OnlineRetail.csv', encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35532748",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_5592\\551935786.py:14: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  basket = basket.applymap(lambda x: 1 if x > 0 else 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total transactions (baskets): 16649\n",
      "Total unique items (after filtering): 3833\n"
     ]
    }
   ],
   "source": [
    "df = df.dropna(subset=['CustomerID', 'Description'])\n",
    "\n",
    "df = df[~df['InvoiceNo'].astype(str).str.startswith('C')]\n",
    "\n",
    "df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'], errors='coerce')\n",
    "\n",
    "df = df[df['Country'] == 'United Kingdom']\n",
    "\n",
    "\n",
    "df['Description'] = df['Description'].str.strip()\n",
    "basket = (df.groupby(['InvoiceNo', 'Description'])['Quantity']\n",
    "          .sum().unstack().fillna(0))\n",
    "\n",
    "basket = basket.applymap(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "transactions = []\n",
    "for invoice, row in basket.iterrows():\n",
    "    items = list(row[row == 1].index)\n",
    "    if items:\n",
    "        transactions.append(frozenset(items))\n",
    "\n",
    "print(f\"Total transactions (baskets): {len(transactions)}\")\n",
    "print(f\"Total unique items (after filtering): {len(basket.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a37baf6",
   "metadata": {},
   "source": [
    "## Step 2: Implement Apriori Algorithm\n",
    "Step-by-Step Procedure:\n",
    "1. Generate Frequent 1-Itemsets\n",
    "Count the frequency (support) of each individual item in the dataset.\n",
    "Keep only those with support â‰¥ min_support.\n",
    "â†’ Result is L1 (frequent 1-itemsets)\n",
    "2. Iterative Candidate Generation (k = 2 to n)\n",
    "While L(k-1) is not empty:\n",
    "a. Candidate Generation\n",
    "\n",
    "Generate candidate itemsets Ck of size k from L(k-1) using the Apriori property:\n",
    "Any (k-itemset) is only frequent if all of its (kâˆ’1)-subsets are frequent.\n",
    "b. Prune Candidates\n",
    "Eliminate candidates that have any (kâˆ’1)-subset not in L(k-1).\n",
    "c. Count Support\n",
    "For each transaction, count how many times each candidate in Ck appears.\n",
    "d. Generate Frequent Itemsets\n",
    "Form Lk by keeping candidates from Ck that meet the min_support.\n",
    "Repeat until Lk becomes empty.\n",
    "Implement the following functions:\n",
    "1. `get_frequent_itemsets(transactions, min_support)` - Returns frequent itemsets and their support\n",
    "2. `generate_candidates(prev_frequent_itemsets, k)` - Generates candidate itemsets of length `k`\n",
    "3. `calculate_support(transactions, candidates)` - Calculates the support count for each candidate\n",
    "\n",
    "**Write reusable functions** for each part of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef97b01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b326dcb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 236 frequent itemsets with min_support=0.02\n",
      "k=1: 200 itemsets\n",
      "k=2: 35 itemsets\n",
      "k=3: 1 itemsets\n"
     ]
    }
   ],
   "source": [
    "def calculate_support(transactions, itemset):\n",
    "    count = 0\n",
    "    for t in transactions:\n",
    "        if itemset.issubset(t):\n",
    "            count += 1\n",
    "    return count / len(transactions)\n",
    "\n",
    "def get_single_item_counts(transactions):\n",
    "    counts = defaultdict(int)\n",
    "    for t in transactions:\n",
    "        for item in t:\n",
    "            counts[frozenset([item])] += 1\n",
    "    return counts\n",
    "\n",
    "def generate_candidates(freq_itemsets_prev, k):\n",
    "    candidates = set()\n",
    "    prev_list = list(freq_itemsets_prev)\n",
    "    n = len(prev_list)\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            a = prev_list[i]\n",
    "            b = prev_list[j]\n",
    "            union = a.union(b)\n",
    "            if len(union) == k:\n",
    "                candidates.add(frozenset(union))\n",
    "    return candidates\n",
    "\n",
    "def prune_candidates(candidates, freq_itemsets_prev, k):\n",
    "    pruned = set()\n",
    "    for c in candidates:\n",
    "        all_subsets_frequent = True\n",
    "        for subset in combinations(c, k-1):\n",
    "            if frozenset(subset) not in freq_itemsets_prev:\n",
    "                all_subsets_frequent = False\n",
    "                break\n",
    "        if all_subsets_frequent:\n",
    "            pruned.add(c)\n",
    "    return pruned\n",
    "\n",
    "def get_frequent_itemsets(transactions, min_support=0.02):\n",
    " \n",
    "    transactions = list(transactions)\n",
    "    n_transactions = len(transactions)\n",
    "    freq_itemsets = dict()\n",
    "    \n",
    "    single_counts = get_single_item_counts(transactions)\n",
    "    L1 = dict()\n",
    "    for itemset, cnt in single_counts.items():\n",
    "        support = cnt / n_transactions\n",
    "        if support >= min_support:\n",
    "            L1[itemset] = support\n",
    "    freq_itemsets[1] = L1\n",
    "    \n",
    "    k = 2\n",
    "    prev_freq_itemsets = set(L1.keys())\n",
    "    while prev_freq_itemsets:\n",
    "        candidates = generate_candidates(prev_freq_itemsets, k)\n",
    "        candidates = prune_candidates(candidates, prev_freq_itemsets, k)\n",
    "        \n",
    "        candidate_counts = defaultdict(int)\n",
    "        for t in transactions:\n",
    "            for c in candidates:\n",
    "                if c.issubset(t):\n",
    "                    candidate_counts[c] += 1\n",
    "        Lk = dict()\n",
    "        for c, cnt in candidate_counts.items():\n",
    "            support = cnt / n_transactions\n",
    "            if support >= min_support:\n",
    "                Lk[c] = support\n",
    "        \n",
    "        if not Lk:\n",
    "            break\n",
    "        \n",
    "        freq_itemsets[k] = Lk\n",
    "        prev_freq_itemsets = set(Lk.keys())\n",
    "        k += 1\n",
    "    \n",
    "    return freq_itemsets\n",
    "\n",
    "min_support = 0.02  \n",
    "freq_itemsets = get_frequent_itemsets(transactions, min_support=min_support)\n",
    "\n",
    "total_itemsets = sum(len(v) for v in freq_itemsets.values())\n",
    "print(f\"Found {total_itemsets} frequent itemsets with min_support={min_support}\")\n",
    "for k in sorted(freq_itemsets):\n",
    "    print(f\"k={k}: {len(freq_itemsets[k])} itemsets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb8c0fe",
   "metadata": {},
   "source": [
    "## Step 3: Generate Association Rules\n",
    "\n",
    "- Use frequent itemsets to generate association rules\n",
    "- For each rule `A => B`, calculate:\n",
    "  - **Support**\n",
    "  - **Confidence**\n",
    "- Only return rules that meet a minimum confidence threshold (e.g., 0.5)\n",
    "\n",
    "ðŸ‘‰ **Implement rule generation function below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5018d896",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "245e6b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 30 rules with min_confidence=0.5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def generate_rules(freq_itemsets, min_confidence=0.5):\n",
    "    \n",
    "    support_lookup = dict()\n",
    "    for k, d in freq_itemsets.items():\n",
    "        for itemset, sup in d.items():\n",
    "            support_lookup[itemset] = sup\n",
    "\n",
    "    rules = []\n",
    "    for k in freq_itemsets:\n",
    "        if k < 2:\n",
    "            continue\n",
    "        for itemset in freq_itemsets[k]:\n",
    "            items = list(itemset)\n",
    "            for r in range(1, len(items)):\n",
    "                for antecedent_tuple in combinations(items, r):\n",
    "                    antecedent = frozenset(antecedent_tuple)\n",
    "                    consequent = itemset - antecedent\n",
    "                    if not consequent:\n",
    "                        continue\n",
    "                    support_itemset = support_lookup.get(itemset, 0)\n",
    "                    support_antecedent = support_lookup.get(antecedent, calculate_support(transactions, antecedent))\n",
    "                    support_consequent = support_lookup.get(consequent, calculate_support(transactions, consequent))\n",
    "                    confidence = support_itemset / support_antecedent if support_antecedent > 0 else 0\n",
    "                    lift = confidence / support_consequent if support_consequent > 0 else 0\n",
    "                    if confidence >= min_confidence:\n",
    "                        rules.append({\n",
    "                            'antecedent': antecedent,\n",
    "                            'consequent': consequent,\n",
    "                            'support': support_itemset,\n",
    "                            'confidence': confidence,\n",
    "                            'lift': lift\n",
    "                        })\n",
    "    rules = sorted(rules, key=lambda x: (x['confidence'], x['lift']), reverse=True)\n",
    "    return rules\n",
    "\n",
    "min_confidence = 0.5\n",
    "rules = generate_rules(freq_itemsets, min_confidence=min_confidence)\n",
    "print(f\"Generated {len(rules)} rules with min_confidence={min_confidence}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf26889",
   "metadata": {},
   "source": [
    "## Step 4: Output and Visualize\n",
    "\n",
    "- Print top 10 frequent itemsets\n",
    "- Print top 10 association rules (by confidence or lift)\n",
    "\n",
    "ðŸ‘‰ **Output results below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0bd38cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2133753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 frequent itemsets:\n",
      "Items (k=1): {'WHITE HANGING HEART T-LIGHT HOLDER'}  --> support: 0.1132\n",
      "Items (k=1): {'JUMBO BAG RED RETROSPOT'}  --> support: 0.0869\n",
      "Items (k=1): {'REGENCY CAKESTAND 3 TIER'}  --> support: 0.0847\n",
      "Items (k=1): {'ASSORTED COLOUR BIRD ORNAMENT'}  --> support: 0.0781\n",
      "Items (k=1): {'PARTY BUNTING'}  --> support: 0.0775\n",
      "Items (k=1): {'LUNCH BAG RED RETROSPOT'}  --> support: 0.0673\n",
      "Items (k=1): {'SET OF 3 CAKE TINS PANTRY DESIGN'}  --> support: 0.0605\n",
      "Items (k=1): {'LUNCH BAG  BLACK SKULL.'}  --> support: 0.0598\n",
      "Items (k=1): {\"PAPER CHAIN KIT 50'S CHRISTMAS\"}  --> support: 0.0568\n",
      "Items (k=1): {'NATURAL SLATE HEART CHALKBOARD'}  --> support: 0.0563\n",
      "\n",
      "Top 10 association rules (by confidence):\n",
      "Rule: {'ROSES REGENCY TEACUP AND SAUCER', 'PINK REGENCY TEACUP AND SAUCER'} -> {'GREEN REGENCY TEACUP AND SAUCER'} | support=0.0205, confidence=0.8903, lift=24.2210\n",
      "Rule: {'GREEN REGENCY TEACUP AND SAUCER', 'PINK REGENCY TEACUP AND SAUCER'} -> {'ROSES REGENCY TEACUP AND SAUCER'} | support=0.0205, confidence=0.8441, lift=20.7268\n",
      "Rule: {'PINK REGENCY TEACUP AND SAUCER'} -> {'GREEN REGENCY TEACUP AND SAUCER'} | support=0.0243, confidence=0.8195, lift=22.2931\n",
      "Rule: {'GREEN REGENCY TEACUP AND SAUCER'} -> {'ROSES REGENCY TEACUP AND SAUCER'} | support=0.0286, confidence=0.7778, lift=19.0991\n",
      "Rule: {'PINK REGENCY TEACUP AND SAUCER'} -> {'ROSES REGENCY TEACUP AND SAUCER'} | support=0.0230, confidence=0.7769, lift=19.0770\n",
      "Rule: {'GARDENERS KNEELING PAD CUP OF TEA'} -> {'GARDENERS KNEELING PAD KEEP CALM'} | support=0.0275, confidence=0.7305, lift=16.3901\n",
      "Rule: {'ROSES REGENCY TEACUP AND SAUCER', 'GREEN REGENCY TEACUP AND SAUCER'} -> {'PINK REGENCY TEACUP AND SAUCER'} | support=0.0205, confidence=0.7164, lift=24.1929\n",
      "Rule: {'ROSES REGENCY TEACUP AND SAUCER'} -> {'GREEN REGENCY TEACUP AND SAUCER'} | support=0.0286, confidence=0.7021, lift=19.0991\n",
      "Rule: {'PINK REGENCY TEACUP AND SAUCER'} -> {'ROSES REGENCY TEACUP AND SAUCER', 'GREEN REGENCY TEACUP AND SAUCER'} | support=0.0205, confidence=0.6917, lift=24.1929\n",
      "Rule: {'RED HANGING HEART T-LIGHT HOLDER'} -> {'WHITE HANGING HEART T-LIGHT HOLDER'} | support=0.0257, confidence=0.6667, lift=5.8914\n"
     ]
    }
   ],
   "source": [
    "\n",
    "all_itemsets = []\n",
    "for k, itm in freq_itemsets.items():\n",
    "    for s, sup in itm.items():\n",
    "        all_itemsets.append((s, sup))\n",
    "all_itemsets_sorted = sorted(all_itemsets, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print('\\nTop 10 frequent itemsets:')\n",
    "for itemset, sup in all_itemsets_sorted[:10]:\n",
    "    print(f\"Items (k={len(itemset)}): {set(itemset)}  --> support: {sup:.4f}\")\n",
    "\n",
    "print('\\nTop 10 association rules (by confidence):')\n",
    "for r in rules[:10]:\n",
    "    print(f\"Rule: {set(r['antecedent'])} -> {set(r['consequent'])} | support={r['support']:.4f}, confidence={r['confidence']:.4f}, lift={r['lift']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04283c52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
